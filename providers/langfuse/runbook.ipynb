{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97cd937",
   "metadata": {},
   "source": [
    "# Migrating from Langfuse to LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c191ad3",
   "metadata": {},
   "source": [
    "\n",
    "## Migrating Resources\n",
    "\n",
    "Contained in this repo are scripts to migrate your resources from Langfuse to LangSmith.\n",
    "\n",
    "This includes:\n",
    "- Datasets\n",
    "- Prompts\n",
    "- Recent Traces\n",
    "\n",
    "To migrate your resources over, refer to ```providers/langfuse/main.py```. Specific scripts for each are provided in the ```providers/langfuse/data``` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ef944",
   "metadata": {},
   "source": [
    "## Updating Code\n",
    "\n",
    "In the process of migrating to LangSmith, you will also need to update your instrumentation code as well. \n",
    "\n",
    "In the following sections we break down some common patterns used in LangFuse, and their equivalent implementation in LangSmith. Not all features are shared, but common constructs are available across both frameworks.\n",
    "\n",
    "First, let's load in our environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60bb23c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64cd8c",
   "metadata": {},
   "source": [
    "### **Tracing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834eeca",
   "metadata": {},
   "source": [
    "#### Observe decorator\n",
    "\n",
    "If you're using the ```@observe``` decorator, the equivalent in LangSmith is the ```@traceable``` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57800622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    " \n",
    "@observe\n",
    "def my_function():\n",
    "    return \"Hello, world!\" # Input/output and timings are automatically captured\n",
    " \n",
    "my_function()\n",
    " \n",
    "# Flush events in short-lived applications\n",
    "langfuse = get_client()\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d88234",
   "metadata": {},
   "source": [
    "Note: with LangSmith you do not need to flush short-lived events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c162af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable\n",
    "def my_function():\n",
    "    return \"Hello, world!\" # Input/output and timings are automatically captured\n",
    "\n",
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54802e1c",
   "metadata": {},
   "source": [
    "#### Context Managers\n",
    "\n",
    "If you're using context managers in LangFuse, LangSmith has an equivalent ls.trace() context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49740681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/2fvdkyfj0856p6_6sdvv74rw0000gn/T/ipykernel_39379/1446926794.py:11: DeprecationWarning: start_as_current_generation is deprecated and will be removed in a future version. Use start_as_current_observation(as_type='generation') instead.\n",
      "  with langfuse.start_as_current_generation(name=\"llm-response\", model=\"gpt-3.5-turbo\") as generation:\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "# Create a span using a context manager\n",
    "with langfuse.start_as_current_span(name=\"process-request\") as span:\n",
    "    # Your processing logic here\n",
    "    span.update(output=\"Processing complete\")\n",
    " \n",
    "    # Create a nested generation for an LLM call\n",
    "    with langfuse.start_as_current_generation(name=\"llm-response\", model=\"gpt-3.5-turbo\") as generation:\n",
    "        # Your LLM call logic here\n",
    "        generation.update(output=\"Generated response\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0f83a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith as ls\n",
    "\n",
    "# Create a trace using the context manager\n",
    "with ls.trace(name=\"process-request\") as rt:\n",
    "    # Your processing logic here\n",
    "    # Create a nested generation for an LLM call\n",
    "    with ls.trace(name=\"llm-response\", run_type=\"llm\", metadata={\"model\": \"gpt-3.5-turbo\"}) as generation:\n",
    "        # Your LLM call logic here\n",
    "        generation.end(outputs={\"output\": \"Generated response\"})\n",
    "\n",
    "    rt.end(outputs={\"output\": \"Processing complete\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8bb9fc",
   "metadata": {},
   "source": [
    "#### OpenTelemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de23d6e",
   "metadata": {},
   "source": [
    "If you're using OpenTelemetry, LangSmith supports [OTel tracing natively.](https://docs.langchain.com/langsmith/trace-with-opentelemetry#trace-with-opentelemetry)\n",
    "\n",
    "You'll likely be switching out the exporter endpoints you had [set with Langfuse](https://langfuse.com/integrations/native/opentelemetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce21a7df",
   "metadata": {},
   "source": [
    "### **Evaluations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c0bb5",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    "In offline evaluations, a dataset is often used to run evaluations over. LangFuse allows you to create a dataset and add examples using the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adc3109e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetItem(id='3d528ef2-d906-46bc-97b9-38628ce64b5c', status=<DatasetStatus.ACTIVE: 'ACTIVE'>, input={'text': 'What is the capital of Germany?'}, expected_output={'text': 'Berlin'}, metadata=None, source_trace_id=None, source_observation_id=None, dataset_id='cmga8r3ll02fuad06uf23cwmf', dataset_name='basic', created_at=datetime.datetime(2025, 10, 3, 2, 44, 50, 788000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 10, 3, 2, 44, 50, 788000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset(\n",
    "    name=\"basic\",\n",
    "    # optional description\n",
    "    description=\"Basic dataset\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")\n",
    "\n",
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"basic\",\n",
    "    # any python object or value, optional\n",
    "    input={\n",
    "        \"text\": \"What is the capital of France?\"\n",
    "    },\n",
    "    # any python object or value, optional\n",
    "    expected_output={\n",
    "        \"text\": \"Paris\"\n",
    "    },\n",
    ")\n",
    "\n",
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"basic\",\n",
    "    input={\n",
    "        \"text\": \"What is the capital of Germany?\"\n",
    "    },\n",
    "    expected_output={\n",
    "        \"text\": \"Berlin\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de09470",
   "metadata": {},
   "source": [
    "LangSmith allows you to create datasets using the LangSmith SDK as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "# Create a dataset\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"expected_output\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of Germany?\",\n",
    "        \"expected_output\": \"Berlin\"\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset_name = \"basic\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    langsmith_dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"input\": ex[\"input\"]} for ex in examples],\n",
    "        outputs=[{\"expected_output\": ex[\"expected_output\"]} for ex in examples],\n",
    "        dataset_id=langsmith_dataset.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558750e3",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510021b",
   "metadata": {},
   "source": [
    "Running experiments with LangFuse in the SDK is done through ```run_experiment```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "283098a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Results: Hidden (2 items)\\nðŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nðŸ§ª Experiment: Multi-metric Evaluation\n",
      "ðŸ“‹ Run name: Multi-metric Evaluation - 2025-10-03T03:12:26.355573Z\\n2 items\\nEvaluations:\\n  â€¢ response_length\\n  â€¢ accuracy\\n\\nAverage Scores:\\n  â€¢ response_length: 36.000\\n  â€¢ accuracy: 1.000\\n\\nðŸ”— Dataset Run:\\n   https://us.cloud.langfuse.com/project/cmg9xbp62008had07ab7us47z/datasets/cmga8r3ll02fuad06uf23cwmf/runs/dbd7a5a7-2a7a-42b4-a01c-606bd5e9eb16\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Evaluation\n",
    "from langfuse.openai import OpenAI\n",
    "\n",
    "# Define your task function\n",
    "def my_task(*, item, **kwargs):\n",
    "    question = item.input[\"text\"]\n",
    "    print(question)\n",
    "    response = OpenAI().chat.completions.create(\n",
    "        model=\"gpt-4.1\", messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    " \n",
    "\n",
    "# Define evaluation functions\n",
    "def accuracy_evaluator(*, input, output, expected_output, **kwargs):\n",
    "    if expected_output and expected_output[\"text\"].lower() in output.lower():\n",
    "        return Evaluation(name=\"accuracy\", value=1.0, comment=\"Correct answer found\")\n",
    "    return Evaluation(name=\"accuracy\", value=0.0, comment=\"Incorrect answer\")\n",
    " \n",
    "def length_evaluator(*, input, output, **kwargs):\n",
    "    return Evaluation(name=\"response_length\", value=len(output), comment=f\"Response has {len(output)} characters\")\n",
    " \n",
    "# Use multiple evaluators\n",
    "dataset = langfuse.get_dataset(\"basic\")\n",
    "result = langfuse.run_experiment(\n",
    "    name=\"Multi-metric Evaluation\",\n",
    "    data=dataset.items,\n",
    "    task=my_task,\n",
    "    evaluators=[accuracy_evaluator, length_evaluator]\n",
    ")\n",
    " \n",
    "print(result.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668553f",
   "metadata": {},
   "source": [
    "The equivalent in LangSmith is using ```evaluate()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0f37aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertxu/Desktop/Projects/experiments/tunnels/tunnel/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'multi-metric-eval-89657962' at:\n",
      "https://smith.langchain.com/o/4015447c-43ab-4414-8539-633d4cb47217/datasets/335a7b19-f7bf-426f-a288-d5b39a5402fb/compare?selectedSessions=4c83a493-5938-487f-9929-1f26fd0c157f\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client, trace\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset = client.read_dataset(dataset_name=\"basic\")\n",
    "\n",
    "# Wrap OpenAI client for tracing\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "# Define your task function\n",
    "def my_task(inputs: dict) -> dict:\n",
    "    question = inputs[\"input\"]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Use a LangSmith-supported model name\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    )\n",
    "    return {\"output\": response.choices[0].message.content}\n",
    "\n",
    "\n",
    "# Define evaluation functions\n",
    "def accuracy_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    output = outputs.get(\"output\", \"\")\n",
    "    expected = reference_outputs.get(\"expected_output\", \"\")\n",
    "    if expected and expected.lower() in output.lower():\n",
    "        return {\"key\": \"accuracy\", \"score\": 1.0, \"comment\": \"Correct answer found\"}\n",
    "    return {\"key\": \"accuracy\", \"score\": 0.0, \"comment\": \"Incorrect answer\"}\n",
    "\n",
    "def length_evaluator(inputs: dict, outputs: dict) -> dict:\n",
    "    output = outputs.get(\"output\", \"\")\n",
    "    return {\"key\": \"response_length\", \"score\": len(output), \"comment\": f\"Response has {len(output)} characters\"}\n",
    "\n",
    "# Run experiment\n",
    "result = client.evaluate(\n",
    "    my_task,\n",
    "    data=dataset.id,\n",
    "    evaluators=[accuracy_evaluator, length_evaluator],\n",
    "    experiment_prefix=\"multi-metric-eval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dafab6",
   "metadata": {},
   "source": [
    "Both LangFuse and LangSmith support flexible evaluation types, including summary evaluators being defined in experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82d900",
   "metadata": {},
   "source": [
    "### **Prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4e61a",
   "metadata": {},
   "source": [
    "LangFuse and LangSmith both have prompting interfaces in the UI and the SDK.\n",
    "\n",
    "LangFuse uses the ```create_prompt``` method, shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "381a92fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langfuse.model.ChatPromptClient at 0x108c19160>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_prompt(\n",
    "    name=\"movie-critic-chat\",\n",
    "    type=\"chat\",\n",
    "    prompt=[\n",
    "      { \"role\": \"system\", \"content\": \"You are an {{criticlevel}} movie critic\" },\n",
    "      { \"role\": \"user\", \"content\": \"Do you like {{movie}}?\" },\n",
    "    ],\n",
    "    labels=[\"production\"],  # directly promote to production\n",
    "    config={\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"supported_languages\": [\"en\", \"fr\"],\n",
    "    },  # optionally, add configs (e.g. model parameters or model tools) or tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8c750",
   "metadata": {},
   "source": [
    "LangSmith has a comparable ```push_prompt``` function in the SDK, which automatically detects which model you're using and includes your exact configuration in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec804c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/movie-critic-chat/4cb87751?organizationId=4015447c-43ab-4414-8539-633d4cb47217'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client = Client()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an {criticlevel} movie critic\"),\n",
    "    (\"human\", \"Do you like {movie}?\")\n",
    "])\n",
    "chain = prompt | model\n",
    "client.push_prompt(\"movie-critic-chat\", object=chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
